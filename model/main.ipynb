{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# create an object of WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# importing the GL Bot corpus file for pre-processing\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open(\"datasets/intent.json\").read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/conradkash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/conradkash/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the json data\n",
    "# tokenization\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 documents\n",
      "50 classes ['acid_reflux', 'allergies', 'alzheimer', 'anxiety', 'appointment', 'arthritis', 'asthma', 'bronchitis', 'cancer', 'common_cold', 'constipation', 'covid', 'depression', 'diabetes', 'diarrhea', 'diet', 'dosage', 'drug_interaction', 'emergency', 'exercise', 'gastroenteritis', 'generic_equivalent', 'greeting', 'headache', 'heart_disease', 'hiv', 'hypertension', 'influenza', 'insomnia', 'insurance', 'kidney_disease', 'medication', 'medication_adherence', 'medication_allergies', 'medication_children', 'medication_pregnancy', 'medication_side_effects', 'medication_storage', 'mental_health', 'migraine', 'osteoporosis', 'parkinson', 'pharmacy', 'pneumonia', 'prescription', 'sinusitis', 'stomach_ulcer', 'stroke', 'symptoms', 'urinary_tract_infection']\n",
      "209 unique lemmatized words [\"'m\", \"'s\", '.', 'a', 'about', 'acid', 'adherence', 'adjust', 'age', 'allergic', 'allergy', 'already', 'alternative', 'alzheimer', 'an', 'and', 'anxiety', 'anxious', 'any', 'appointment', 'are', 'arthritis', 'asthma', 'at', 'attack', 'attention', 'available', 'avoid', 'balanced', 'bathroom', 'be', 'become', 'beginner', 'between', 'beverage', 'book', 'brand', 'bronchitis', 'can', 'cancer', 'case', 'cause', 'certain', 'child', 'cold', 'common', 'concerned', 'constipation', 'copay', 'cost', 'cough', 'cover', 'covid-19', 'depression', 'diabetes', 'diarrhea', 'diet', 'difference', 'disease', 'do', 'doe', 'dosage', 'dose', 'drug', 'during', 'effect', 'effective', 'effectively', 'emergency', 'equivalent', 'exercise', 'factor', 'feeling', 'fever', 'flu', 'food', 'for', 'from', 'gastroenteritis', 'generic', 'get', 'go', 'have', 'headache', 'healthy', 'heart', 'hello', 'hey', 'hi', 'hiv', 'hospital', 'how', 'hypertension', 'i', 'identify', 'if', 'improve', 'in', 'infection', 'influenza', 'insomnia', 'insurance', 'interaction', 'is', 'keep', 'kidney', 'known', 'lab', 'last', 'long', 'lose', 'manage', 'meal', 'medical', 'medication', 'mental', 'migraine', 'miss', 'multiple', 'my', 'myself', 'nearby', 'next', 'of', 'often', 'on', 'online', 'or', 'osteoporosis', 'other', 'out', 'own', 'parkinson', 'plan', 'planning', 'pneumonia', 'potential', 'pregnancy', 'pregnant', 'prescription', 'prevent', 'prevention', 'procedure', 'protect', 'quality', 'reaction', 'recognize', 'recommend', 'recommended', 'reduce', 'refill', 'reflux', 'relaxation', 'relieve', 'remedy', 'request', 'requirement', 'restriction', 'risk', 'routine', 'run', 'same', 'schedule', 'seek', 'should', 'side', 'sign', 'sinusitis', 'sleep', 'some', 'specialist', 'specific', 'stomach', 'storage', 'store', 'strategy', 'stressed', 'stroke', 'suggest', 'symptom', 'take', 'taking', 'technique', 'test', 'the', 'there', 'this', 'time', 'to', 'tract', 'treatment', 'type', 'typically', 'ulcer', 'urinary', 'use', 'usually', 'valid', 'version', 'visit', 'weight', 'well-being', 'what', 'when', 'while', 'with', 'woman', 'workout', 'you']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize, lower each word and remove duplicates\n",
    "\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
